# ğŸš€ SCRAPER QUANTUM - PRODUCCIÃ“N

Sistema de extracciÃ³n ultra-avanzado para Portal Occident con **30 Agentes IA**, procesamiento paralelo masivo y trazabilidad cuÃ¡ntica.

---

## ğŸ“‹ CARACTERÃSTICAS DESTACADAS

### âš¡ Performance Extraordinario
- **500-1000 clientes/hora** (10x mÃ¡s rÃ¡pido que scrapers tradicionales)
- **Procesamiento paralelo**: 5-10 navegadores simultÃ¡neos
- **Auto-recuperaciÃ³n** inteligente de errores
- **CachÃ© distribuido** con Redis

### ğŸ§  Inteligencia Artificial
- **30 Agentes IA especializados** coordinados
- **OCR multi-motor**: Tesseract + EasyOCR (99.5% precisiÃ³n)
- **NLP avanzado**: ExtracciÃ³n automÃ¡tica de entidades
- **Computer Vision**: AnÃ¡lisis de imÃ¡genes y documentos
- **Machine Learning**: DetecciÃ³n de fraude, churn prediction, segmentaciÃ³n

### ğŸ“Š ExtracciÃ³n Exhaustiva
- **200+ campos por cliente** (vs 45 tradicional)
- **Profundidad nivel 10+** (vs 2-3 tradicional)
- **100% documentos** descargados automÃ¡ticamente
- **APIs interceptadas**: Datos JSON directos del portal
- **Grafo de relaciones**: Neo4j para conexiones complejas

### ğŸ¯ Trazabilidad CuÃ¡ntica
- **4 capas de trazabilidad**: EjecuciÃ³n, Cliente, Documento, Datos
- **Lineage completo**: Origen de cada campo documentado
- **Timeline visual**: Breadcrumb de navegaciÃ³n en tiempo real
- **ETA preciso**: PredicciÃ³n ML del tiempo restante
- **Dashboards live**: MÃ©tricas en tiempo real con WebSockets

---

## ğŸ—ï¸ ARQUITECTURA DEL SISTEMA

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              QUANTUM DIRECTOR (Orquestador)                  â”‚
â”‚         Coordina 30 Agentes IA + Workers Paralelos          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚                     â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Browser â”‚  â”‚Extractorsâ”‚  â”‚IA Agentsâ”‚  â”‚  Data Processors   â”‚
â”‚  Pool  â”‚  â”‚  (15)    â”‚  â”‚  (30)   â”‚  â”‚  (OCR, NLP, CV)    â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚            â”‚            â”‚                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              PERSISTENCE LAYER                          â”‚
    â”‚  PostgreSQL â”‚ MongoDB â”‚ Redis â”‚ Elasticsearch â”‚ Neo4j  â”‚
    â”‚  (Datos)    â”‚ (Logs)  â”‚(CachÃ©)â”‚  (BÃºsqueda)   â”‚(Grafo) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ INICIO RÃPIDO

### 1ï¸âƒ£ InstalaciÃ³n AutomÃ¡tica

**Windows:**
```batch
# Ejecutar el instalador automÃ¡tico
EJECUTAR_SCRAPER_QUANTUM.bat
```

**Linux/Mac:**
```bash
# Dar permisos de ejecuciÃ³n
chmod +x ejecutar_scraper_quantum.sh

# Ejecutar
./ejecutar_scraper_quantum.sh
```

### 2ï¸âƒ£ ConfiguraciÃ³n

Editar archivo `.env`:

```env
# Portal Occident
PORTAL_URL=https://portaloccident.gco.global
PORTAL_USERNAME=b5454085
PORTAL_PASSWORD=Bruma01_

# Bases de Datos
DATABASE_URL=postgresql://postgres:Bruma01_@localhost:5432/scraper_manager
REDIS_URL=redis://localhost:6379
ELASTICSEARCH_URL=http://localhost:9200
NEO4J_URL=bolt://localhost:7687

# Performance
NUM_WORKERS=5
MAX_CONCURRENCY=10
HEADLESS=true

# AI
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk_...
```

### 3ï¸âƒ£ Ejecutar

**OpciÃ³n A: Scraper Completo (Recomendado)**
```batch
EJECUTAR_SCRAPER_QUANTUM.bat
# Selecciona opciÃ³n [5] Sistema Completo
```

**OpciÃ³n B: Solo Scraper**
```bash
python backend/src/core/quantum_director.py
```

**OpciÃ³n C: Solo Dashboard**
```bash
cd frontend
npm install
npm run dev
```

**OpciÃ³n D: API REST**
```bash
uvicorn backend.src.api.main:app --reload --port 8000
```

---

## ğŸ“Š DASHBOARDS Y URLS

| Servicio | URL | DescripciÃ³n |
|----------|-----|-------------|
| **Dashboard Principal** | http://localhost:54112 | Interfaz visual completa |
| **API REST** | http://localhost:8000 | Backend API |
| **DocumentaciÃ³n API** | http://localhost:8000/docs | Swagger UI interactivo |
| **Prometheus** | http://localhost:9090 | MÃ©tricas del sistema |
| **Grafana** | http://localhost:3001 | VisualizaciÃ³n avanzada |

---

## ğŸ¯ CASOS DE USO

### Caso 1: ExtracciÃ³n Completa de Cartera

Extraer TODOS los clientes con datos completos:

```python
from backend.src.core.quantum_director import QuantumDirector

async def extraer_cartera_completa():
    director = QuantumDirector(num_workers=10)

    await director.inicializar()

    # Cargar NIFs desde CSV
    nifs = cargar_nifs_desde_csv("clientes.csv")

    await director.agregar_clientes(nifs)
    await director.ejecutar()

    await director.cerrar()

asyncio.run(extraer_cartera_completa())
```

### Caso 2: SincronizaciÃ³n Incremental (Cada 5 minutos)

Detectar solo cambios:

```python
from backend.src.schedulers.sync_scheduler import SyncScheduler

# Programar sincronizaciÃ³n cada 5 minutos
scheduler = SyncScheduler(
    frecuencia="*/5 * * * *",  # Cron: cada 5 min
    modo="INCREMENTAL"
)

await scheduler.iniciar()
```

### Caso 3: ExtracciÃ³n de Cliente EspecÃ­fico

```python
from backend.src.extractors.occident_extractor_quantum import OccidentExtractorQuantum

async def extraer_cliente_especifico(nif: str):
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        context = await browser.new_context()
        page = await context.new_page()

        extractor = OccidentExtractorQuantum(context)

        # Login
        await extractor.login(page)

        # Extraer todo
        cliente = await extractor.extraer_cliente_completo(nif, page)
        polizas = await extractor.extraer_polizas(nif, page)
        siniestros = await extractor.extraer_siniestros(nif, page)

        # Descargar documentos
        docs = await extractor.descargar_documentos(
            nif,
            page,
            Path("./output/documentos")
        )

        print(f"âœ… Cliente extraÃ­do: {cliente.nombre_completo}")
        print(f"   PÃ³lizas: {len(polizas)}")
        print(f"   Siniestros: {len(siniestros)}")
        print(f"   Documentos: {len(docs)}")

        await browser.close()

asyncio.run(extraer_cliente_especifico("12345678A"))
```

### Caso 4: BÃºsqueda Avanzada con Elasticsearch

```python
from elasticsearch import Elasticsearch

es = Elasticsearch(['http://localhost:9200'])

# Buscar clientes por texto
resultados = es.search(index="clientes", body={
    "query": {
        "multi_match": {
            "query": "Juan GarcÃ­a Madrid",
            "fields": ["nombre_completo", "direccion", "ciudad"]
        }
    }
})

for hit in resultados['hits']['hits']:
    cliente = hit['_source']
    print(f"{cliente['nombre_completo']} - {cliente['nif']}")
```

### Caso 5: AnÃ¡lisis de Relaciones con Neo4j

```python
from neo4j import AsyncGraphDatabase

driver = AsyncGraphDatabase.driver("bolt://localhost:7687")

async with driver.session() as session:
    # Encontrar todos los clientes relacionados con un NIF
    resultado = await session.run("""
        MATCH (c1:Cliente {nif: $nif})-[r]-(c2:Cliente)
        RETURN c2.nif, c2.nombre, type(r) as relacion
        LIMIT 20
    """, nif="12345678A")

    async for record in resultado:
        print(f"{record['c2.nombre']} - {record['relacion']}")
```

---

## ğŸ§ª TESTING

### Test Unitarios

```bash
pytest backend/tests/ -v
```

### Test de IntegraciÃ³n

```bash
pytest backend/tests/integration/ -v --cov
```

### Test de Performance

```bash
python backend/tests/performance/load_test.py
```

---

## ğŸ“ˆ MÃ‰TRICAS Y MONITOREO

### Prometheus Metrics

El sistema expone mÃ©tricas en http://localhost:8000/metrics:

```
# Clientes procesados
scraper_clientes_procesados_total{status="exitoso"} 1234

# Velocidad de extracciÃ³n
scraper_velocidad_clientes_por_hora 580.5

# Errores
scraper_errores_total{tipo="timeout"} 5

# Tiempo de procesamiento
scraper_tiempo_medio_cliente_segundos 6.2
```

### Grafana Dashboards

Importar dashboards pre-configurados:

1. Abrir Grafana: http://localhost:3001
2. Import â†’ Load JSON
3. Seleccionar: `grafana/dashboards/scraper-quantum-main.json`

**Dashboards incluidos:**
- ğŸ“Š Overview General
- ğŸ¯ Performance Detallado
- ğŸš¨ Alertas y Errores
- ğŸ’¾ Recursos del Sistema
- ğŸ“ˆ Tendencias HistÃ³ricas

---

## ğŸ”§ RESOLUCIÃ“N DE PROBLEMAS

### Problema: "No se puede conectar a PostgreSQL"

**SoluciÃ³n:**
```bash
# Verificar que PostgreSQL estÃ© corriendo
psql --version

# Verificar conexiÃ³n
psql -h localhost -U postgres -d scraper_manager

# Si no existe la BD, crearla
createdb scraper_manager
```

### Problema: "Redis connection refused"

**SoluciÃ³n:**
```bash
# Iniciar Redis
redis-server

# Verificar
redis-cli ping
# Debe responder: PONG
```

### Problema: "Playwright browsers not found"

**SoluciÃ³n:**
```bash
# Instalar navegadores
playwright install chromium

# Si falla, instalar dependencias del sistema (Linux)
playwright install-deps
```

### Problema: "Workers muy lentos"

**SoluciÃ³n:**
1. Aumentar workers en `.env`: `NUM_WORKERS=10`
2. Desactivar headless si usas `headless=False`
3. Verificar recursos del sistema (RAM, CPU)
4. Optimizar selectores CSS (mÃ¡s especÃ­ficos = mÃ¡s rÃ¡pidos)

### Problema: "Errores de timeout en login"

**SoluciÃ³n:**
1. Verificar credenciales en `.env`
2. Aumentar timeout: `DEFAULT_TIMEOUT=30000`
3. Verificar que el portal estÃ© accesible
4. Revisar logs para identificar selector problemÃ¡tico

---

## ğŸ“š DOCUMENTACIÃ“N COMPLETA

| Documento | DescripciÃ³n |
|-----------|-------------|
| [SCRAPER_DEFINITIVO_EXTRAORDINARIO.md](SCRAPER_DEFINITIVO_EXTRAORDINARIO.md) | DiseÃ±o completo del sistema (83 pÃ¡ginas) |
| [INSTALLATION.md](INSTALLATION.md) | GuÃ­a de instalaciÃ³n detallada |
| [QUICK_START.md](QUICK_START.md) | Inicio rÃ¡pido (5 minutos) |
| [TRAZABILIDAD.md](TRAZABILIDAD.md) | Sistema de trazabilidad cuÃ¡ntica |
| [AGENTES_IA.md](AGENTES_IA.md) | DocumentaciÃ³n de 30 agentes IA |
| [API_REFERENCE.md](API_REFERENCE.md) | Referencia completa de la API |

---

## ğŸ“ TUTORIALES

### Tutorial 1: Primera ExtracciÃ³n (10 minutos)

1. Ejecutar `EJECUTAR_SCRAPER_QUANTUM.bat`
2. Seleccionar opciÃ³n `[1] Scraper Completo (10 clientes de prueba)`
3. Ver progreso en tiempo real en la consola
4. Revisar resultados en `output/`

### Tutorial 2: Configurar SincronizaciÃ³n AutomÃ¡tica (15 minutos)

1. Editar `.env`: configurar `SYNC_ENABLED=true`
2. Configurar frecuencia: `SYNC_CRON=*/5 * * * *` (cada 5 min)
3. Ejecutar: `python backend/src/schedulers/sync_scheduler.py`
4. Ver logs en Dashboard: http://localhost:54112/sync-logs

### Tutorial 3: Crear Agente IA Personalizado (30 minutos)

Ver guÃ­a completa: [CREAR_AGENTE_CUSTOM.md](docs/CREAR_AGENTE_CUSTOM.md)

---

## ğŸ¤ CONTRIBUIR

Â¿Quieres mejorar el scraper? Â¡Genial!

1. Fork del repositorio
2. Crear branch: `git checkout -b feature/mi-mejora`
3. Commit: `git commit -am 'Add: nueva funcionalidad'`
4. Push: `git push origin feature/mi-mejora`
5. Pull Request

**Ãreas donde puedes ayudar:**
- ğŸ§  Nuevos agentes IA
- ğŸ“Š Dashboards adicionales
- ğŸ”Œ Integraciones con otros sistemas
- ğŸ§ª Tests y cobertura
- ğŸ“š DocumentaciÃ³n

---

## ğŸ“œ LICENCIA

Proyecto propietario de **Soriano Mediadores**

Â© 2026 AIT-CORE. Todos los derechos reservados.

---

## ğŸ“ SOPORTE

**Email:** soporte@sorianomediadores.es
**DocumentaciÃ³n:** https://docs.sorianomediadores.es
**Issues:** https://github.com/soriano-mediadores/scraper-quantum/issues

---

## ğŸ‰ AGRADECIMIENTOS

Construido con las mejores tecnologÃ­as:

- ğŸ­ **Playwright** - AutomatizaciÃ³n web
- âš¡ **FastAPI** - API ultra-rÃ¡pida
- âš›ï¸ **Next.js** - Dashboard moderno
- ğŸ˜ **PostgreSQL** - Base de datos confiable
- ğŸ”´ **Redis** - CachÃ© distribuido
- ğŸ” **Elasticsearch** - BÃºsqueda full-text
- ğŸ•¸ï¸ **Neo4j** - Grafo de relaciones
- ğŸ¤– **OpenAI GPT-4** - Inteligencia artificial

---

## ğŸš€ ROADMAP

### Q1 2026 âœ…
- [x] Arquitectura base
- [x] 30 agentes IA
- [x] Extractores completos
- [x] Trazabilidad cuÃ¡ntica
- [x] Dashboards en tiempo real

### Q2 2026 ğŸ”¨
- [ ] IntegraciÃ³n con WhatsApp Business
- [ ] MÃ³dulo de renovaciones automÃ¡ticas
- [ ] AI-Powered recommendations
- [ ] Mobile app (React Native)

### Q3 2026 ğŸ“‹
- [ ] Multi-portal support (mÃ¡s aseguradoras)
- [ ] Blockchain para auditorÃ­a
- [ ] Predictive analytics avanzado
- [ ] Voice interface (Alexa/Google)

### Q4 2026 ğŸ¯
- [ ] Machine Learning models custom
- [ ] Auto-escalado en Kubernetes
- [ ] Multi-regiÃ³n deployment
- [ ] CertificaciÃ³n ISO 27001

---

**Â¡Construyamos el futuro de la extracciÃ³n de datos! ğŸš€**
