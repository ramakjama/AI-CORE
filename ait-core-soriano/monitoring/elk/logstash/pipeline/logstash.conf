# Logstash Pipeline Configuration for AIT-CORE

input {
  # Docker logs via Filebeat
  beats {
    port => 5044
    ssl => false
    # For production, enable SSL:
    # ssl => true
    # ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
    # ssl_certificate => "/etc/logstash/certs/logstash.crt"
    # ssl_key => "/etc/logstash/certs/logstash.key"
  }

  # Syslog
  syslog {
    port => 5000
    type => "syslog"
  }

  # Application logs via TCP
  tcp {
    port => 5001
    codec => json_lines
    type => "application"
  }

  # HTTP input for custom logs
  http {
    port => 8080
    codec => json
    type => "http"
  }

  # Kafka (if using message queue)
  # kafka {
  #   bootstrap_servers => "kafka:9092"
  #   topics => ["application-logs", "system-logs"]
  #   codec => json
  #   consumer_threads => 4
  #   group_id => "logstash-ait-core"
  # }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed"
    }

    # Move parsed fields to root level
    ruby {
      code => "
        if event.get('parsed')
          event.get('parsed').each { |k, v|
            event.set(k, v) unless event.include?(k)
          }
          event.remove('parsed')
        end
      "
    }
  }

  # Grok patterns for various log formats
  grok {
    match => {
      "message" => [
        # Express/Node.js logs
        "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{GREEDYDATA:message}",
        # NGINX access logs
        "%{IPORHOST:clientip} - %{USER:user} \[%{HTTPDATE:timestamp}\] \"%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:status} %{NUMBER:bytes} \"%{DATA:referrer}\" \"%{DATA:useragent}\"",
        # Default pattern
        "%{GREEDYDATA:unparsed_message}"
      ]
    }
    break_on_match => true
  }

  # Parse timestamp
  date {
    match => [
      "timestamp",
      "ISO8601",
      "yyyy-MM-dd HH:mm:ss",
      "dd/MMM/yyyy:HH:mm:ss Z"
    ]
    target => "@timestamp"
    remove_field => ["timestamp"]
  }

  # Add application metadata
  if [container_name] {
    mutate {
      add_field => {
        "application" => "%{container_name}"
      }
    }

    # Extract service name from container name
    grok {
      match => {
        "container_name" => "^/?(?<service>[^_-]+)"
      }
    }
  }

  # Normalize log levels
  if [level] {
    mutate {
      uppercase => ["level"]
    }
  }

  # GeoIP lookup for IP addresses
  if [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }

  # User agent parsing
  if [useragent] {
    useragent {
      source => "useragent"
      target => "user_agent"
    }
  }

  # Add tags based on conditions
  if [level] == "ERROR" or [level] == "FATAL" {
    mutate {
      add_tag => ["error"]
    }
  }

  if [status] >= 500 {
    mutate {
      add_tag => ["http_error"]
    }
  }

  if [status] >= 400 and [status] < 500 {
    mutate {
      add_tag => ["http_client_error"]
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["beat", "input", "agent", "ecs", "host"]
  }

  # Handle errors
  if "_grokparsefailure" in [tags] {
    mutate {
      add_field => {
        "parse_error" => "grok_parse_failure"
      }
    }
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"

    # Dynamic index naming based on log type and date
    index => "logs-%{[service]:unknown}-%{+YYYY.MM.dd}"

    # Index lifecycle management
    ilm_enabled => true
    ilm_rollover_alias => "logs"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "ait-core-logs-policy"

    # Template
    template_name => "ait-core-logs"
    template_overwrite => true
  }

  # Output errors to separate index
  if "error" in [tags] or "_grokparsefailure" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      index => "errors-%{+YYYY.MM.dd}"
    }
  }

  # Debug output (comment out in production)
  # stdout {
  #   codec => rubydebug
  # }

  # Send metrics to Prometheus (via push gateway)
  # http {
  #   url => "http://pushgateway:9091/metrics/job/logstash"
  #   http_method => "post"
  #   format => "json"
  # }
}
